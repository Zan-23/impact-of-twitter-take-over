{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dddd1f16",
   "metadata": {},
   "source": [
    "## Preprocessing text for the Detoxify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1652aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly \n",
    "import plotly.graph_objects as go\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from detoxify import Detoxify\n",
    "import spacy\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# download this via \"python -m spacy download en_core_web_sm\"\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "try:\n",
    "    print(run_only_once)\n",
    "except Exception as e:\n",
    "    print(os.getcwd())\n",
    "    os.chdir(\"./../../\")\n",
    "    print(os.getcwd())\n",
    "    run_only_once = \"Dir has already been changed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bfab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols to remove\n",
    "emoji_regex = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "\n",
    "def prepare_text_for_tweet_file(input_file, output_file_name):\n",
    "    print(f\"\\nStarted generating toxicity metrics for:\\n\"\n",
    "      f\"-input file: '{input_file}',\\n\"\n",
    "      f\"-output file: '{output_file_name}'\")\n",
    "    start_time = time.time()\n",
    "    tweets_df = pd.read_csv(\"./data/raw_hashtags/\" + input_file)\n",
    "    total_len = len(tweets_df.index)\n",
    "    \n",
    "    tweets_df = tweets_df[tweets_df[\"lang\"] == \"en\"]\n",
    "    print(f\"Removed {total_len - len(tweets_df.index)} tweets out of {len(tweets_df.index)}, since they were not in English\\n\")\n",
    "    # if we don't do it, the toxicity metrics will missmatch down the line\n",
    "    tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    # add new column for processed text\n",
    "    tweets_df[\"text_processed\"] = np.nan\n",
    "    preprocessed_text_arr = []\n",
    "    for index, row in tweets_df.iterrows():\n",
    "        if index % 500 == 0:\n",
    "            print(f\"At row: {index}/{len(tweets_df.index)}\")\n",
    "        # remove emojis       \n",
    "        text = emoji_regex.sub(r'', row[\"text\"])    \n",
    "        # lematize it using spacy\n",
    "        lemmas = ' '.join([x.lemma_ for x in nlp(text)])\n",
    "        \n",
    "        # add it to the array which will be added as a column at the end\n",
    "        preprocessed_text_arr.append(lemmas)\n",
    " \n",
    "    tweets_df[\"text_processed\"] = pd.Series(preprocessed_text_arr) \n",
    "    print(\"Finished lemmitization\")\n",
    "    \n",
    "    tweets_df.to_csv(output_file_name, header=True)\n",
    "    print(f\"\\nExecution took: {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Finished saving to file '{output_file_name}'\\n\")\n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized version\n",
    "def optimized_prepare_text_for_tweet_file(input_file, output_file_name):\n",
    "    print(f\"\\nStarted generating toxicity metrics for:\\n\"\n",
    "      f\"-input file: '{input_file}',\\n\"\n",
    "      f\"-output file: '{output_file_name}'\")\n",
    "    start_time = time.time()\n",
    "    tweets_df = pd.read_csv(\"./data/raw_hashtags/\" + input_file)\n",
    "    total_len = len(tweets_df.index)\n",
    "    \n",
    "    tweets_df = tweets_df[tweets_df[\"lang\"] == \"en\"]\n",
    "    print(f\"Removed {total_len - len(tweets_df.index)} tweets out of {len(tweets_df.index)}, since they were not in English\\n\")\n",
    "    # if we don't do it, the toxicity metrics will missmatch down the line\n",
    "    tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "    # removed emoji and other weird symbols\n",
    "    symbol_removed_col = tweets_df[\"text\"].str.replace(emoji_regex, \"\")\n",
    "    # remove new lines, tabs, and multiple spaces.\n",
    "    symbol_removed_col = symbol_removed_col.str.replace(r'\\r+|\\n+|\\t+','', regex=True).replace(r'\\s+', ' ', regex=True)\n",
    " \n",
    "    # lematize it using spacy\n",
    "    # lemmas = ' '.join([x.lemma_ for x in nlp(text)])\n",
    "    # array in which to store the data\n",
    "    preprocessed_text_arr = []\n",
    "    \n",
    "    lemmas_arr = []\n",
    "    # perform multithreaded execution\n",
    "    for doc in tqdm(nlp.pipe(symbol_removed_col.astype(\"unicode\").values, batch_size=15, n_process=3)):\n",
    "        if doc.has_annotation:\n",
    "            # contains actual word/token\n",
    "            # tokens.append([n.text for n in doc])\n",
    "            # contains the label of the token\n",
    "            # pos.append([n.pos_ for n in doc])\n",
    "            \n",
    "            # contains the lemmatized sentence\n",
    "            lemmas_arr.append(\" \".join([token.lemma_ for token in doc]))\n",
    "        else:\n",
    "            # We want to make sure that the lists of parsed results have the\n",
    "            # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "            tokens.append(None)\n",
    "            lemma.append(None)\n",
    "            pos.append(None)\n",
    "\n",
    "    tweets_df['processed_lemmas'] = pd.Series(lemmas_arr) \n",
    "    # tweets_df[\"text_processed\"] = pd.Series(preprocessed_text_arr) \n",
    "    print(\"Finished lemmitization\")\n",
    "    \n",
    "    tweets_df.to_csv(output_file_name, header=True)\n",
    "    print(f\"\\nExecution took: {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Finished saving to file '{output_file_name}'\\n\")\n",
    "    # return tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca0136a",
   "metadata": {},
   "source": [
    "## Run Lemmatization\n",
    "This will run lemmatization  on text for all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4419b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files that we want to prepare\n",
    "hashtag_files = [\"vegetarian_hashtag_6_1_2023.csv\", \"uno_hashtag_09_01_2023.csv\", \n",
    "                 \"vegan_hashtag_6_1_2023.csv\", \"fitness_hashtag_08_01_2023.csv\", \"netflix_hashtag_08_01_2023.csv\", \n",
    "                 \"musk_hashtag_03_01_2023.csv\", \"trump_hashtag_13_01_2023.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66368c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in hashtag_files:\n",
    "    output_file = f\"./data/lemmatized/{file.split('.')[0]}_lemmatized.csv\"\n",
    "    # tmp_df = prepare_text_for_tweet_file(file, output_file)\n",
    "    optimized_prepare_text_for_tweet_file(file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8666a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d649e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2114e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b69b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
