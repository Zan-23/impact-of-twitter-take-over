{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dddd1f16",
   "metadata": {},
   "source": [
    "## Preprocessing text for the Detoxify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70040f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zan\\Desktop\\Faksic_TUM\\3_semester\\seminar_social_computing\\impact-of-twitter-take-over\\notebooks\\zan\n",
      "C:\\Users\\Zan\\Desktop\\Faksic_TUM\\3_semester\\seminar_social_computing\\impact-of-twitter-take-over\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly \n",
    "import plotly.graph_objects as go\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from detoxify import Detoxify\n",
    "import spacy\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# download this via \"python -m spacy download en_core_web_sm\"\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "try:\n",
    "    print(run_only_once)\n",
    "except Exception as e:\n",
    "    print(os.getcwd())\n",
    "    os.chdir(\"./../../\")\n",
    "    print(os.getcwd())\n",
    "    run_only_once = \"Dir has already been changed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5bfab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols to remove\n",
    "emoji_regex = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\" # TODO this line should be removed/modified else netflix hashtag won't work\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "\n",
    "def prepare_text_for_tweet_file(input_file, output_file_name):\n",
    "    print(f\"\\nStarted generating toxicity metrics for:\\n\"\n",
    "      f\"-input file: '{input_file}',\\n\"\n",
    "      f\"-output file: '{output_file_name}'\")\n",
    "    start_time = time.time()\n",
    "    tweets_df = pd.read_csv(\"./data/raw_hashtags/\" + input_file)\n",
    "    total_len = len(tweets_df.index)\n",
    "    \n",
    "    tweets_df = tweets_df[tweets_df[\"lang\"] == \"en\"]\n",
    "    print(f\"Removed {total_len - len(tweets_df.index)} tweets out of {len(tweets_df.index)}, since they were not in English\\n\")\n",
    "    # if we don't do it, the toxicity metrics will missmatch down the line\n",
    "    tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    # add new column for processed text\n",
    "    tweets_df[\"text_processed\"] = np.nan\n",
    "    preprocessed_text_arr = []\n",
    "    for index, row in tweets_df.iterrows():\n",
    "        if index % 500 == 0:\n",
    "            print(f\"At row: {index}/{len(tweets_df.index)}\")\n",
    "        # remove emojis       \n",
    "        text = emoji_regex.sub(r'', row[\"text\"])    \n",
    "        # lematize it using spacy\n",
    "        lemmas = ' '.join([x.lemma_ for x in nlp(text)])\n",
    "        \n",
    "        # add it to the array which will be added as a column at the end\n",
    "        preprocessed_text_arr.append(lemmas)\n",
    " \n",
    "    tweets_df[\"text_processed\"] = pd.Series(preprocessed_text_arr) \n",
    "    print(\"Finished lemmitization\")\n",
    "    \n",
    "    tweets_df.to_csv(output_file_name, header=True)\n",
    "    print(f\"\\nExecution took: {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Finished saving to file '{output_file_name}'\\n\")\n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed04bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized version\n",
    "def optimized_prepare_text_for_tweet_file(input_file, output_file_name):\n",
    "    print(f\"\\nStarted generating toxicity metrics for:\\n\"\n",
    "      f\"-input file: '{input_file}',\\n\"\n",
    "      f\"-output file: '{output_file_name}'\")\n",
    "    start_time = time.time()\n",
    "    tweets_df = pd.read_csv(\"./data/raw_hashtags/\" + input_file)\n",
    "    total_len = len(tweets_df.index)\n",
    "    \n",
    "    tweets_df = tweets_df[tweets_df[\"lang\"] == \"en\"]\n",
    "    print(f\"Removed {total_len - len(tweets_df.index)} tweets out of {len(tweets_df.index)}, since they were not in English\\n\")\n",
    "    # if we don't do it, the toxicity metrics will missmatch down the line\n",
    "    tweets_df = tweets_df.reset_index(drop=True)\n",
    "    \n",
    "    # removed emoji and other weird symbols\n",
    "    symbol_removed_col = tweets_df[\"text\"].str.replace(emoji_regex, \"\")\n",
    "    # remove new lines, tabs, and multiple spaces.\n",
    "    symbol_removed_col = symbol_removed_col.str.replace(r'\\r+|\\n+|\\t+','', regex=True).replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "    # array in which to store the data\n",
    "    preprocessed_text_arr = []\n",
    "    \n",
    "    lemmas_arr = []\n",
    "    # perform multithreaded execution\n",
    "    for doc in tqdm(nlp.pipe(symbol_removed_col.astype(\"unicode\").values, batch_size=15, n_process=3)):\n",
    "        if doc.has_annotation:\n",
    "            # contains actual word/token\n",
    "            # tokens.append([n.text for n in doc])\n",
    "            # contains the label of the token\n",
    "            # pos.append([n.pos_ for n in doc])\n",
    "            \n",
    "            # contains the lemmatized sentence\n",
    "            lemmas_arr.append(\" \".join([token.lemma_ for token in doc]))\n",
    "        else:\n",
    "            # We want to make sure that the lists of parsed results have the\n",
    "            # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "            tokens.append(None)\n",
    "            lemma.append(None)\n",
    "            pos.append(None)\n",
    "\n",
    "    tweets_df['processed_text'] = pd.Series(lemmas_arr) \n",
    "    print(\"Finished lemmitization\")\n",
    "    \n",
    "    tweets_df.to_csv(output_file_name, header=True)\n",
    "    print(f\"\\nExecution took: {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Finished saving to file '{output_file_name}'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d5c34",
   "metadata": {},
   "source": [
    "## Run Lemmatization\n",
    "This will run lemmatization  on text for all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4419b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files that we want to prepare\n",
    "hashtag_files = [\"vegetarian_hashtag_6_1_2023.csv\", \"uno_hashtag_09_01_2023.csv\", \n",
    "                 \"vegan_hashtag_6_1_2023.csv\", \"fitness_hashtag_08_01_2023.csv\", \"netflix_hashtag_08_01_2023.csv\", \n",
    "                 \"musk_hashtag_03_01_2023.csv\", \"trump_hashtag_13_01_2023.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f66368c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started generating toxicity metrics for:\n",
      "-input file: 'vegetarian_hashtag_6_1_2023.csv',\n",
      "-output file: './data/lemmatized/vegetarian_hashtag_6_1_2023_lemmatized.csv'\n",
      "Removed 30 tweets out of 71134, since they were not in English\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71134it [01:50, 643.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished lemmitization\n",
      "\n",
      "Execution took: 112.79 seconds\n",
      "Finished saving to file './data/lemmatized/vegetarian_hashtag_6_1_2023_lemmatized.csv'\n",
      "\n",
      "\n",
      "Started generating toxicity metrics for:\n",
      "-input file: 'uno_hashtag_09_01_2023.csv',\n",
      "-output file: './data/lemmatized/uno_hashtag_09_01_2023_lemmatized.csv'\n",
      "Removed 1429 tweets out of 107575, since they were not in English\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "107575it [02:15, 794.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished lemmitization\n",
      "\n",
      "Execution took: 138.08 seconds\n",
      "Finished saving to file './data/lemmatized/uno_hashtag_09_01_2023_lemmatized.csv'\n",
      "\n",
      "\n",
      "Started generating toxicity metrics for:\n",
      "-input file: 'vegan_hashtag_6_1_2023.csv',\n",
      "-output file: './data/lemmatized/vegan_hashtag_6_1_2023_lemmatized.csv'\n",
      "Removed 151 tweets out of 248143, since they were not in English\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "248143it [05:34, 741.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished lemmitization\n",
      "\n",
      "Execution took: 341.70 seconds\n",
      "Finished saving to file './data/lemmatized/vegan_hashtag_6_1_2023_lemmatized.csv'\n",
      "\n",
      "\n",
      "Started generating toxicity metrics for:\n",
      "-input file: 'fitness_hashtag_08_01_2023.csv',\n",
      "-output file: './data/lemmatized/fitness_hashtag_08_01_2023_lemmatized.csv'\n",
      "Removed 1076 tweets out of 280376, since they were not in English\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280376it [06:54, 676.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished lemmitization\n",
      "\n",
      "Execution took: 423.53 seconds\n",
      "Finished saving to file './data/lemmatized/fitness_hashtag_08_01_2023_lemmatized.csv'\n",
      "\n",
      "\n",
      "Started generating toxicity metrics for:\n",
      "-input file: 'netflix_hashtag_08_01_2023.csv',\n",
      "-output file: './data/lemmatized/netflix_hashtag_08_01_2023_lemmatized.csv'\n",
      "Removed 8433 tweets out of 1637171, since they were not in English\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1637171it [32:53, 829.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished lemmitization\n",
      "\n",
      "Execution took: 2018.51 seconds\n",
      "Finished saving to file './data/lemmatized/netflix_hashtag_08_01_2023_lemmatized.csv'\n",
      "\n",
      "\n",
      "Started generating toxicity metrics for:\n",
      "-input file: 'musk_hashtag_03_01_2023.csv',\n",
      "-output file: './data/lemmatized/musk_hashtag_03_01_2023_lemmatized.csv'\n",
      "Removed 1024 tweets out of 742395, since they were not in English\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "742395it [17:49, 694.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished lemmitization\n",
      "\n",
      "Execution took: 1092.98 seconds\n",
      "Finished saving to file './data/lemmatized/musk_hashtag_03_01_2023_lemmatized.csv'\n",
      "\n",
      "\n",
      "Started generating toxicity metrics for:\n",
      "-input file: 'trump_hashtag_13_01_2023.csv',\n",
      "-output file: './data/lemmatized/trump_hashtag_13_01_2023_lemmatized.csv'\n",
      "Removed 1165 tweets out of 2362363, since they were not in English\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2362363it [1:00:45, 648.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished lemmitization\n",
      "\n",
      "Execution took: 3727.66 seconds\n",
      "Finished saving to file './data/lemmatized/trump_hashtag_13_01_2023_lemmatized.csv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in hashtag_files:\n",
    "    output_file = f\"./data/lemmatized/{file.split('.')[0]}_lemmatized.csv\"\n",
    "    optimized_prepare_text_for_tweet_file(file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8666a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d649e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2114e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b69b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
