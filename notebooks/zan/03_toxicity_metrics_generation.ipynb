{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a5459f",
   "metadata": {},
   "source": [
    "# Toxicity metrics data generation\n",
    "In this notebook I generate toxicity metrics with the Detoxify library, which is used to measure toxicity of texts, in our case tweets.   \n",
    "   \n",
    "This is meant as a suplementary approach to the Perpective API since we were limited by the number of queries when using it.\n",
    "\n",
    "Make sure to install CUDA achieves at least 5x speed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f85e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly  \n",
    "import plotly.graph_objects as go\n",
    "import time\n",
    "import random\n",
    "# progress monitoring\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_splitter import split_text_into_sentences\n",
    "# for dealing with messed up data\n",
    "from unidecode import unidecode\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "from detoxify import Detoxify\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    print(run_only_once)\n",
    "except Exception as e:\n",
    "    print(os.getcwd())\n",
    "    os.chdir(\"./../../\")\n",
    "    print(os.getcwd())\n",
    "    run_only_once = \"Dir has already been changed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a302ab77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clear memory to reduce memory errors\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "# test if cuda is available, it has to be otherwise slow asf, 33 hours for 1.2 million tweets \n",
    "device = torch.device(\"cuda\")\n",
    "cuda_present = torch.cuda.is_available()\n",
    "print(f\"Cuda present: {cuda_present}\")\n",
    "\n",
    "# load the model\n",
    "model = Detoxify('original', device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2202c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for single predictions testing\n",
    "# model.predict(\"Love on the Spectrum is the cutest show on Netflix rn ðŸ¥¹ðŸ’“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed3084",
   "metadata": {},
   "source": [
    "## Test how the text is split into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for sentence splitting\n",
    "sentences = split_text_into_sentences(\n",
    "    text='This is a paragraph. It contains several sentences. \"But why,\" you ask?',\n",
    "    language='en'\n",
    ")\n",
    "\n",
    "for sent in sentences:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f577661",
   "metadata": {},
   "source": [
    "## Generating toxicity scores for each tweet\n",
    "The code below needed 19092 seconds (5.3 hours) to run the last time, with CUDA on ~1 million tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6796954b",
   "metadata": {},
   "source": [
    "This function is the initial implementation of toxicity generation, which was used when we didn't do any preprocessing on the text.\n",
    "The code of this function is kept for help, not for actually using it!  \n",
    "\n",
    "```from src.toxicity_metric_generation_functions import upgraded_generate_toxicity_for_tweet_file```   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba7093",
   "metadata": {},
   "source": [
    "Here is the new implementation of the function above which split the text into sentences and computes the averages over them, and it also generally more robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.toxicity_metric_generation_functions import upgraded_generate_toxicity_for_tweet_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfe859f",
   "metadata": {},
   "source": [
    "Here we actually run our toxify method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58bc94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hashtag_files_lemmatized = [\"vegetarian_hashtag_6_1_2023_lemmatized.csv\", \"netflix_hashtag_08_01_2023_lemmatized.csv\", \n",
    "                            \"uno_hashtag_09_01_2023_lemmatized.csv\", \"vegan_hashtag_6_1_2023_lemmatized.csv\", \n",
    "                            \"fitness_hashtag_08_01_2023_lemmatized.csv\", \"musk_hashtag_03_01_2023_lemmatized.csv\", \n",
    "                            \"trump_hashtag_13_01_2023_lemmatized.csv\"]\n",
    "# for trump check that the right file was added, since data was collected later \"trump_hashtag_13_01_2023.csv\"\n",
    "\n",
    "# to not override files by mistake\n",
    "hash_int = random.randrange(1000)\n",
    "for file_name in hashtag_files_lemmatized:\n",
    "    replaced_str = file_name.replace('.csv', '').replace('_lemmatized', '')\n",
    "    output_file = f\"./data/detoxify_toxicity_added_hashtags/lemmatized_{replaced_str}_detoxify_toxicity_{hash_int}.csv\"\n",
    "    print(f\"Saving to {output_file}\")\n",
    "    \n",
    "    upgraded_generate_toxicity_for_tweet_file(model=model, input_file=file_name, output_file=output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5521109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5064175c",
   "metadata": {},
   "source": [
    "## Unicode\n",
    "We used **unidecode** function to automatically converts a string to be more asci compliant. Problem occured at netflix on line ~595628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68accf1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
