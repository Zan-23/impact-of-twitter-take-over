{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a5459f",
   "metadata": {},
   "source": [
    "# Toxicity metrics data generation\n",
    "In this notebook I generate toxicity metrics with the Detoxify library which is used to measure toxicity of texts, in our case tweets. \n",
    "This is meant as a suplementary approach to the Perpective API since we are limited by the number of queries when using it.\n",
    "\n",
    "Make sure to install CUDA achieves at least 5x speed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f85e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly \n",
    "import plotly.graph_objects as go\n",
    "import time\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from detoxify import Detoxify\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    print(run_only_once)\n",
    "except Exception as e:\n",
    "    print(os.getcwd())\n",
    "    os.chdir(\"./../../\")\n",
    "    print(os.getcwd())\n",
    "    run_only_once = \"Dir has already been changed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a302ab77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# clear memory to reduce memory errors\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "# test if cuda is available, it has to be otherwise slow asf, 33 hours for 1.2 million tweets \n",
    "device = torch.device(\"cuda\")\n",
    "cuda_present = torch.cuda.is_available()\n",
    "print(f\"Cuda present: {cuda_present}\")\n",
    "\n",
    "# load the model\n",
    "model = Detoxify('original', device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2202c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for single predictions\n",
    "model.predict(\"Love on the Spectrum is the cutest show on Netflix rn ðŸ¥¹ðŸ’“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbce06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove\n",
    "\"\"\"\n",
    "tweets_df = pd.read_csv(\"./data/twitter_1_million_tweet_dump_29_12_2022.csv\")\n",
    "total_len = len(tweets_df.index)\n",
    "tweets_df = tweets_df[tweets_df[\"lang\"] == \"en\"]\n",
    "print(f\"Removed {total_len - len(tweets_df.index)} tweets out of {len(tweets_df.index)}, since they were not in English\")\n",
    "\n",
    "# if we don't do it, the toxicity metrics will missmatch down the line\n",
    "tweets_df = tweets_df.reset_index(drop=True)\n",
    "tweets_df\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f577661",
   "metadata": {},
   "source": [
    "## Generating toxicity scores for each tweet\n",
    "The code below needed 19092 seconds (5.3 hours) to run the last time, with CUDA on ~1 million tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c31b91a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TODO add lemization of the sentences to the script\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_text_into_sentences\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# this below does stemming\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# TODO add lemization of the sentences to the script\n",
    "import spacy\n",
    "from sentence_splitter import split_text_into_sentences\n",
    "\n",
    "# this below does stemming\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentences_lemmas = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    lemmas = nlp(sentence)\n",
    "    lemmas = ' '.join([x.lemma_ for x in lemmas]) \n",
    "    sentences_lemmas.append(lemmas)\n",
    "    \n",
    "for sent, lemma in zip(sentences, sentences_lemmas):\n",
    "    print(f\"\\\"{sent}\\\" turns into \\\"{lemma}\\\"\")\n",
    "    \n",
    "# this splits sentences\n",
    "sentences = split_text_into_sentences(\n",
    "    text='This is a paragraph. It contains several sentences. \"But why,\" you ask?',\n",
    "    language='en'\n",
    ")\n",
    "\n",
    "for sent in sentences:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc956a7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO move this to .py file\n",
    "def generate_toxicity_for_tweet_file(input_file, output_file_name):\n",
    "    print(f\"\\nStarted generating toxicity metrics for:\\n\"\n",
    "      f\"-input file: '{input_file}',\\n\"\n",
    "      f\"-output file: '{output_file_name}'\")\n",
    "    tweets_df = pd.read_csv(\"./data/raw_hashtags/\" + input_file)\n",
    "    total_len = len(tweets_df.index)\n",
    "    tweets_df = tweets_df[tweets_df[\"lang\"] == \"en\"]\n",
    "    print(f\"Removed {total_len - len(tweets_df.index)} tweets out of {len(tweets_df.index)}, since they were not in English\")\n",
    "\n",
    "    # if we don't do it, the toxicity metrics will missmatch down the line\n",
    "    tweets_df = tweets_df.reset_index(drop=True)\n",
    "    print(\"Tweet df:\")\n",
    "    display(tweets_df.head(5))\n",
    "\n",
    "    # generating toxicity scores for each tweet\n",
    "    start_time = time.time() \n",
    "    csv_columns = list(tweets_df.columns) + [\"toxicity\", \"severe_toxicity\", \"obscene\", \"threat\", \"insult\", \"identity_attack\"]\n",
    "    toxicity_df = pd.DataFrame(columns=csv_columns)\n",
    "    # save headers to file\n",
    "    toxicity_df.to_csv(output_file_name)\n",
    "    content_list = tweets_df[\"text\"].to_list()\n",
    "    \n",
    "    # multi step - it should work fine now! You can use it and it should be a bit faster\n",
    "    step = 50\n",
    "    for i in range(0, len(tweets_df.index), step):\n",
    "        if i % 500 == 0 and i != 0:\n",
    "            print(f\"At row: {i}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            toxicity_df.to_csv(output_file_name, mode='a', header=False)\n",
    "            print(\"Cleared GPU cache and saved to file\")\n",
    "            toxicity_df = pd.DataFrame(columns=csv_columns)\n",
    "            \n",
    "        curr_tox_dict = model.predict(content_list[i:i+step])\n",
    "        curr_tweet_dict = tweets_df.iloc[i:i+step].reset_index(drop=True).to_dict(orient=\"list\")\n",
    "        merged_tweet_tox = pd.merge(pd.DataFrame(curr_tweet_dict), pd.DataFrame(curr_tox_dict), \n",
    "                                    left_index=True, right_index=True)\n",
    "        toxicity_df = pd.concat([toxicity_df, merged_tweet_tox], ignore_index=True)\n",
    "        \n",
    "    toxicity_df.to_csv(output_file_name, mode='a', header=False)\n",
    "    print(f\"Execution took: {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Finished saving to file '{output_file_name}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58bc94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hashtag_files = [\"vegetarian_hashtag_6_1_2023.csv\", \"trump_hashtag_04_01_2023.csv\", \"uno_hashtag_09_01_2023.csv\", \n",
    "#                 \"vegan_hashtag_6_1_2023.csv\", \"fitness_hashtag_08_01_2023.csv\", \"musk_hashtag_03_01_2023.csv\",\n",
    "#                \"netflix_hashtag_08_01_2023.csv\"]\n",
    "hashtag_files = [\"musk_hashtag_03_01_2023.csv\", \"netflix_hashtag_08_01_2023.csv\"]\n",
    "\n",
    "# to not override files by mistake\n",
    "hash_int = random.randrange(1000)\n",
    "for file_name in hashtag_files:\n",
    "    output_file = f\"./data/detoxify_toxicity_added_hashtags/{file_name.replace('.csv', '')}_detoxify_toxicity_{hash_int}.csv\"\n",
    "    generate_toxicity_for_tweet_file(file_name, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5521109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not needed anymore since the merge is done in the loop already\n",
    "\"\"\"\n",
    "# load the csv with the toxicity data\n",
    "tox_df = pd.read_csv(csv_file_n)\n",
    "tox_df = tox_df.drop(\"Unnamed: 0\", axis=1)\n",
    "tweets_df = tweets_df.reset_index(drop=True)\n",
    "display(tox_df)\n",
    "display(tweets_df)\n",
    "\n",
    "merged_f = \"merged_single_pred_toxicity_7_1.csv\"\n",
    "# merge tweets with toxicity and save it to a file\n",
    "merged_toxic_df = pd.merge(tweets_df, tox_df, left_index=True, right_index=True)\n",
    "merged_toxic_df.to_csv(merged_f)\n",
    "display(merged_toxic_df)\n",
    "\n",
    "# merged_toxic_df.iloc[1136439] \n",
    "\"\"\"\n",
    "# testing problems with toxicity\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# display(tox_df.iloc[1136439])\n",
    "# display(tweets_df.iloc[1136439])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68accf1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6473066b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e704781d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac08598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b2eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
