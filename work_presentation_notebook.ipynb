{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62799c1c",
   "metadata": {},
   "source": [
    "# Report: Has twitter become more toxic?\n",
    "*Å½an Stanonik, Damla Cinel, Lorenz Mangold*   \n",
    "-> TODO spell check the whole thing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249dde91",
   "metadata": {},
   "source": [
    "In the light of the recent events at the beginning of the seminar Computational Social Science, our team decided to take a look into how the take over of Twitter by Elon Musk impacted the platform (acquisition happened on 27.10.2022).   \n",
    "We specifically focused on toxicity, since it was most discussed in the media and a lot of people hinted at increase in racism, bullying and other forms of toxicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f4a8b",
   "metadata": {},
   "source": [
    "## 1. Theory and data selection\n",
    "We couldn't just download all the Tweets from a given point in time to the given date, due to the limitations of the Twitter API and the lack of compute and storage resources on our side.    \n",
    "\n",
    "Keeping these limitations in mind, we decided to only look at certain \"subsets\" of Twitter data, which we approximated with hashtags/keywords. We collected only the Tweets that fulfilled the following criteria:\n",
    "1. They contained the following hashtags/keywords: \n",
    "  * trump\n",
    "  * musk \n",
    "  * fitness\n",
    "  * netflix\n",
    "  * vegan\n",
    "  * vegeterian\n",
    "  * uno\n",
    "2. They were in English\n",
    "3. They weren't retweets\n",
    "4. They didn't have links\n",
    "5. They weren't replies \n",
    "6. They weren't quoting other tweets   \n",
    "   \n",
    "With these limitations we wanted to removed tweets that we couldn't process properly, tweets that were posted by bots, tweets out of context, and scammer tweets with links to other webpages. Our hope was that with these limitation we would only gather tweets where people truly voice there opinions and we could gather all of them.\n",
    "\n",
    "We only looked at the time period from 01/06/2022 to 03/01/2023 which resulted in 216 days of data collection, so that we could really gather all the tweets on this data subset since we were limited with the Twitter Academic API (and its 10 million tweets).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a285da7",
   "metadata": {},
   "source": [
    "## 2. Data gathering \n",
    "Working on the limitations and specifics we set in the previous chapter we researched how we can actually gather the Tweets that we are interested in. We tested out various libraries such as [Tweepy](https://www.tweepy.org/), but we found out it was too limiting for us. Hence we decided to design our own function for retriving the tweets, and it resulted in the following main function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.twitter_download_functions import download_tweets\n",
    "\n",
    "search_query = \"( vegan OR #vegan ) lang:en -is:retweet -is:quote -has:links -is:reply\"\n",
    "download_tweets(search_query=search_query, \n",
    "                tweets_per_day=18000, \n",
    "                start_time=\"01/06/2022 00:00\", \n",
    "                end_time=\"03/01/2023 00:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd3e79",
   "metadata": {},
   "source": [
    "With this kind of set up we specified the query that should be passed to the Twitter Academic in the function parameter `search_query`, while the other parameters were processed on our side locally and added additional information to the query.   \n",
    "\n",
    "The information they added was mainly the time from which the tweets should be gathered, since our method split the day into `tweets_per_day` // 500 intervals, because the current API documentation said that it is unable to provide more than 500 tweets per request. In this case the day would be split into 36 equal time intervals.\n",
    "   \n",
    "Another limitation of the Twitter API is also that no more than 300 request can be sent in a 15 minute interval so the data gathering took a few days for all the tweets to be collected. Our final dataset was then comprised of 7 data files with a combined size of 1.14GB and contained ~5.5 million tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d259b3",
   "metadata": {},
   "source": [
    "## 3. Text preprocessing \n",
    "\n",
    "Now we had all the data that we needed, and we first feed it directly to the Detoxify model, but we realized this was bad practice and decided to preprocess text of each tweet to make the predictions more robust and to discard any weird symbols in the text. \n",
    "   \n",
    "The text was preprocessed in the following order of steps:  \n",
    "1. First we replaced all the weird symbols with non standard charaters such as emojis, non-english letters etc. with the help of regex.\n",
    "2. Then we removed multiple spaces and new lines in strings to have continuous sentences.\n",
    "3. We gave each tweet to the spacy 'en_core_web_sm' model which lemmatized the words into their base forms.  \n",
    "\n",
    "All of the steps mentioned were packaged in a function used below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567074f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, re\n",
    "from src.text_preparation import optimized_prepare_text_for_tweet_file\n",
    "\n",
    "test_reg = re.compile(\"[\"\n",
    "                      u\"\\U0001F600-\\U0001F64F\" \n",
    "                      u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "optimized_prepare_text_for_tweet_file(replace_symbols_regex=test_reg, \n",
    "                                      input_file=\"vegetarian_hashtag_6_1_2023.csv\", \n",
    "                                      output_file_name=\"test_vegetarian_hashtag_6_1_2023.csv\", \n",
    "                                      nlp_model=spacy.load('en_core_web_sm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02df2cce",
   "metadata": {},
   "source": [
    "We executed the following function on all the tweets that we gathered and saved the preprocessed text as an additional column in the csv files which resulted in data increasing to 1.9GB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4059a9d6",
   "metadata": {},
   "source": [
    "## 4. Toxicity metric generation\n",
    "\n",
    "### 4.1 Perspective API\n",
    "Intially our goal was to use the [Perspective API](https://perspectiveapi.com/) offered by Google to acquire the toxicity metrics from the tweets we gathered. It turned out that the API is pretty limited in the amount of request per second you can request from it, and even though we received an increase of request per second, we calculated that we would need at least 8 days to get the metrics for all 5.5 million tweets. Therefore we abandoned this approach due to time limitations, and additional work that would be needed to make it possible to send more request per second to the API.\n",
    "\n",
    "-> TODO Lorenz add something if you think it is needed\n",
    "\n",
    "\n",
    "### 4.2 Detoxify library\n",
    "When we abandoned the initial apprach with the Perspective API we began searching for alternatives and we found the [Detoxify python library](https://pypi.org/project/detoxify/) which seemed straight forward to use, and was built on top of models who won multiple Kaggle competitions.\n",
    "\n",
    "We then designed a function which used the bert base uncased model provided by the library to predict the toxicity metric. When feeding the tweets to the model we made sure to split each tweet into sentences and aquire each sentence toxicity separately, and averaged the toxicity over all the sentences for each tweet to make the predictions more robust.\n",
    "\n",
    "Due to the large amount of data we were working with we added intermidiate saving of the tweets and the metrics during the execution to reduce the memory load, and we also loaded the model to the GPU with the help of CUDA which speed up the execution by a factor of 5x. An example of the developed function can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38111210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detoxify import Detoxify\n",
    "from src.toxicity_metric_generation_functions import upgraded_generate_toxicity_for_tweet_file\n",
    "\n",
    "upgraded_generate_toxicity_for_tweet_file(model=Detoxify(\"original\", device=\"cuda\"), \n",
    "                                          input_file=\"vegetarian_hashtag_6_1_2023_lemmatized.csv\", \n",
    "                                          output_file=\"vegetarian_hashtag_6_1_2023_lemm_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1495392b",
   "metadata": {},
   "source": [
    "This was ran for all the tweets in our data set, one time for the raw texts and one time for the preprocessed lemmatized text. We did this to compare the results and to see what kind of effect did the preprocessing of text had on the general change in toxicity metrics. \n",
    "With the added metrics we were now up to 2.59GB of data for both texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8b1a5",
   "metadata": {},
   "source": [
    "## 5. Analysis \n",
    "-> TODO start from here add results charts etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdba44",
   "metadata": {},
   "source": [
    "## 6. Conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af10a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
